{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "FAISS_INDEX_PATH = os.getenv(\"FAISS_INDEX_PATH\", \"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain 라이브러리에서 필요한 모듈을 가져옵니다.\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "# ABC 모듈을 가져와서 추상 클래스와 메서드를 정의합니다.\n",
    "from abc import ABC, abstractmethod\n",
    "# itemgetter를 가져와서 딕셔너리에서 값을 쉽게 가져올 수 있도록 합니다.\n",
    "from operator import itemgetter\n",
    "\n",
    "# RetrievalChain이라는 추상 클래스를 정의합니다.\n",
    "class RetrievalChain(ABC):\n",
    "    # 초기화 메서드입니다. source_uri를 인자로 받습니다.\n",
    "    def __init__(self, source_uri):\n",
    "        self.source_uri = None  # 문서의 출처를 저장할 변수입니다.\n",
    "        self.k = 5  # 검색할 문서의 수를 설정합니다.\n",
    "        self.cached_embeddings = None\n",
    "\n",
    "    # 문서를 로드하는 추상 메서드입니다. 이 메서드는 하위 클래스에서 구현해야 합니다.\n",
    "    @abstractmethod\n",
    "    def load_documents(self, source_uris):\n",
    "        \"\"\"loader를 사용하여 문서를 로드합니다.\"\"\"\n",
    "        pass\n",
    "\n",
    "    # 텍스트 분할기를 생성하는 추상 메서드입니다. 하위 클래스에서 구현해야 합니다.\n",
    "    @abstractmethod\n",
    "    def create_text_splitter(self):\n",
    "        \"\"\"text splitter를 생성합니다.\"\"\"\n",
    "        pass\n",
    "\n",
    "    # 문서를 분할하는 메서드입니다. 주어진 문서와 텍스트 분할기를 사용합니다.\n",
    "    def split_documents(self, docs, text_splitter):\n",
    "        \"\"\"text splitter를 사용하여 문서를 분할합니다.\"\"\"\n",
    "        return text_splitter.split_documents(docs)\n",
    "\n",
    "    # 임베딩을 생성하는 메서드입니다.\n",
    "    def create_embedding(self):\n",
    "        #upstage 임베딩 사용하는 경우 - 이게 토큰당 비용이 더 싸고 무료 크레딧 제공하긴 합니다\n",
    "        #embedding = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "        #openai 임베딩 사용하는 경우\n",
    "        embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "        fs = LocalFileStore(\"./cached_embeddings/\")\n",
    "        self.cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "            embedding, fs, namespace=embedding.model\n",
    "        )\n",
    "        return self.cached_embeddings\n",
    "\n",
    "    # 벡터 저장소를 생성하는 메서드입니다.\n",
    "    def create_vectorstore(self, split_docs,cache=True):\n",
    "        if cache:\n",
    "            if os.path.exists(FAISS_INDEX_PATH):\n",
    "                print(\"기존 FAISS 인덱스를 로드합니다.\")\n",
    "                return FAISS.load_local(FAISS_INDEX_PATH, self.cached_embeddings, allow_dangerous_deserialization=True)\n",
    "            else:\n",
    "                print(\"새로운 FAISS 벡터 저장소를 생성합니다.\")\n",
    "                vector_store = FAISS.from_texts(split_docs, self.cached_embeddings)\n",
    "                vector_store.save_local(FAISS_INDEX_PATH)\n",
    "                return vector_store\n",
    "        return FAISS.from_documents(\n",
    "            documents=split_docs, embedding=self.create_embedding()\n",
    "        )\n",
    "\n",
    "    # 검색을 수행하는 retriever를 생성하는 메서드입니다.\n",
    "    def create_retriever(self, vectorstore):\n",
    "        # MMR을 사용하여 검색을 수행하는 retriever를 생성합니다.\n",
    "        dense_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\", search_kwargs={\"k\": self.k}\n",
    "        )\n",
    "        return dense_retriever\n",
    "\n",
    "    # 모델을 생성하는 메서드입니다.\n",
    "    def create_model(self):\n",
    "        return ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # 프롬프트를 생성하는 메서드입니다.\n",
    "    def create_prompt(self):\n",
    "        # 프롬프트 템플릿 생성 ! 이부분을 적절히 수정하여 사용하시면 됩니다.\n",
    "        prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "        return prompt\n",
    "\n",
    "    # 문서를 포맷하는 정적 메서드입니다.\n",
    "    @staticmethod\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\".join(docs)\n",
    "    # format_docs 함수 정의\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\".join(\n",
    "            [\n",
    "                f\"<document><content>{doc.page_content}</content><source>{doc.metadata.get('source', '정보 없음')}</source><page>{int(doc.metadata.get('page', 0)) + 1}</page></document>\"\n",
    "                for doc in docs\n",
    "            ]\n",
    "        )\n",
    "    # 전체 체인을 생성하는 메서드입니다.\n",
    "    def create_chain(self):\n",
    "        docs = self.load_documents(self.source_uri)  # 문서를 로드합니다.\n",
    "        text_splitter = self.create_text_splitter()  # 텍스트 분할기를 생성합니다.\n",
    "        split_docs = self.split_documents(docs, text_splitter)  # 문서를 분할합니다.\n",
    "        self.vectorstore = self.create_vectorstore(split_docs)  # 벡터 저장소를 생성합니다.\n",
    "        self.retriever = self.create_retriever(self.vectorstore)  # retriever를 생성합니다.\n",
    "        model = self.create_model()  # 모델을 생성합니다.\n",
    "        prompt = self.create_prompt()  # 프롬프트를 생성합니다.\n",
    "        # 체인을 구성합니다.\n",
    "        self.chain = (\n",
    "        {\"context\":itemgetter(\"context\"), \"question\":itemgetter(\"question\")} # 컨텍스트와 질문을 연결합니다.\n",
    "            | prompt  # 프롬프트와 연결합니다.\n",
    "            | model  # 모델과 연결합니다.\n",
    "            | StrOutputParser()  # 출력 파서를 연결합니다.\n",
    "        )\n",
    "        return self  # 현재 인스턴스를 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "#해야할 것\n",
    "# 1. sql 쿼리 받아온 데이터를 json 형태로 변환해서 디렉토리에 저장하기\n",
    "# 2. 데이터 전처리 하기 -> 문서 전처리 함수 만들기 (DB 데이터 형식에 맞춰서 변환 바랍니다)\n",
    "# 3. 벡터 저장소 만들기 -> 벡터 저장소 함수 만들기\n",
    "# 4. 벡터 저장소 업로드 하기 -> 벡터 저장소 업로드 함수 만들기\n",
    "\n",
    "# JSONRetrievalChain 클래스는 RetrievalChain 클래스를 상속받아 JSON 파일에서 문서를 로드하는 기능을 제공합니다.\n",
    "class JSONRetrievalChain(RetrievalChain):\n",
    "    def __init__(self, source_uri):\n",
    "        self.source_uri = source_uri  # 데이터 소스 URI를 초기화합니다.\n",
    "        self.k = 5  # 검색할 문서의 수를 설정합니다.\n",
    "        self.cached_embeddings = super().create_embedding()\n",
    "    def sql_query(self):\n",
    "        # 이 부분에서 sql로 받아온 데이터를 변환해서 디렉토리에 json형태로 저장하셔야합니다\n",
    "        # 아래는 gpt 코드라 안해봐서 참고만 하십셔\n",
    "\n",
    "        # import sqlite3  # SQLite 데이터베이스를 다루기 위해 sqlite3 모듈을 임포트합니다.\n",
    "        # import json  # JSON 파일을 다루기 위해 json 모듈을 임포트합니다.\n",
    "\n",
    "        # # 데이터베이스에 연결합니다.\n",
    "        # conn = sqlite3.connect('your_database.db')  # 데이터베이스 파일 경로를 지정합니다.\n",
    "        # cursor = conn.cursor()  # 커서를 생성합니다.\n",
    "\n",
    "        # # SQL 쿼리를 실행하여 데이터를 가져옵니다.\n",
    "        # cursor.execute(\"SELECT * FROM documents WHERE category = '법률'\")  # 법률 문서를 검색하는 SQL 쿼리\n",
    "        # rows = cursor.fetchall()  # 쿼리 결과를 가져옵니다.\n",
    "\n",
    "        # # 결과를 JSON 형식으로 변환합니다.\n",
    "        # documents = []\n",
    "        # for row in rows:\n",
    "        #     document = {\n",
    "        #         'book_id': row[0],  # 첫 번째 열을 book_id로 사용합니다.\n",
    "        #         'text': row[1],  # 두 번째 열을 텍스트로 사용합니다.\n",
    "        #         'category': row[2],  # 세 번째 열을 카테고리로 사용합니다.\n",
    "        #         'popularity': row[3],  # 네 번째 열을 인기도로 사용합니다.\n",
    "        #         'keyword': row[4],  # 다섯 번째 열을 키워드로 사용합니다.\n",
    "        #         'word_segment': row[5],  # 여섯 번째 열을 단어 분할로 사용합니다.\n",
    "        #         'publication_ymd': row[6]  # 일곱 번째 열을 출판 날짜로 사용합니다.\n",
    "        #     }\n",
    "        #     documents.append(document)  # 문서를 리스트에 추가합니다.\n",
    "\n",
    "        # # JSON 파일로 저장합니다.\n",
    "        # with open('output_documents.json', 'w', encoding='utf-8') as json_file:\n",
    "        #     json.dump({'data': documents}, json_file, ensure_ascii=False, indent=4)  # JSON 파일로 저장합니다.\n",
    "\n",
    "        # cursor.close()  # 커서를 닫습니다.\n",
    "        # conn.close()  # 데이터베이스 연결을 닫습니다.\n",
    "\n",
    "        return \"SELECT * FROM documents WHERE category = '법률'\"  # 법률 문서를 검색하는 SQL 쿼리를 반환합니다.\n",
    "    \n",
    "    # load_documents 메서드는 주어진 JSON 파일에서 문서를 로드합니다.\n",
    "    def load_documents(self, source_uris):\n",
    "        # JSON 파일의 경로를 설정합니다.\n",
    "        data_dir = \"data/154.의료, 법률 전문 서적 말뭉치/01-1.정식개방데이터/Training/02.라벨링데이터/Training_legal.json\"  # data 폴더에 json 파일들을 저장해 주세요\n",
    "        import json  # JSON 파일을 다루기 위해 json 모듈을 임포트합니다.\n",
    "        split_docs = []  # 문서를 저장할 리스트를 초기화합니다.\n",
    "\n",
    "        # JSON 파일을 열고 데이터를 읽습니다.\n",
    "        with open(data_dir, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                json_data = json.load(f)  # JSON 파일을 파싱하여 데이터를 로드합니다.\n",
    "                            \n",
    "                # 'data' 키가 있는지 확인하고, 해당 배열에서 문서를 추출합니다.\n",
    "                if isinstance(json_data, dict) and 'data' in json_data:\n",
    "                    for index, item in enumerate(json_data['data']):  # JSON 데이터에서 문서를 순회합니다.\n",
    "                        if index >= 10:  # 20번째 문서까지만 로드\n",
    "                            break\n",
    "                        if isinstance(item, dict):  # 각 항목이 딕셔너리인지 확인합니다.\n",
    "                            # Document 객체를 생성하여 문서 내용을 저장합니다.\n",
    "                            from langchain.schema import Document\n",
    "                            doc = Document(\n",
    "                                page_content=item.get('text', ''),  # 문서의 텍스트를 가져옵니다.\n",
    "                                metadata={  # 문서의 메타데이터를 설정합니다.\n",
    "                                    'book_id': item.get('book_id'),  # 책 ID\n",
    "                                    'category': item.get('category'),  # 카테고리\n",
    "                                    'popularity': item.get('popularity'),  # 인기\n",
    "                                    'keyword': item.get('keyword', []),  # 키워드\n",
    "                                    'word_segment': item.get('word_segment', []),  # 단어 분할\n",
    "                                    'publication_ymd': item.get('publication_ymd')  # 출판 날짜\n",
    "                                }\n",
    "                            )\n",
    "                            split_docs.append(doc)  # 생성한 Document 객체를 리스트에 추가합니다.\n",
    "                        else:\n",
    "                            print(\"JSON 데이터가 예상된 형식이 아닙니다.\")  # 데이터 형식 오류 메시지\n",
    "                            print(\"데이터 구조:\", json_data.keys() if isinstance(json_data, dict) else type(json_data))  # 데이터 구조 출력\n",
    "                    \n",
    "                    print(f\"전체 {len(json_data['data'])}개 중 {len(split_docs)}개의 문서를 로드했습니다.\")  # 로드된 문서 수 출력\n",
    "                                \n",
    "            except json.JSONDecodeError as e:  # JSON 파싱 중 오류가 발생한 경우\n",
    "                print(f\"JSON 파일 파싱 중 오류가 발생했습니다: {e}\")  # 오류 메시지 출력\n",
    "        return split_docs  # 로드된 문서 리스트를 반환합니다.\n",
    "\n",
    "    # create_text_splitter 메서드는 텍스트 분할기를 생성합니다.\n",
    "    def create_text_splitter(self):\n",
    "        return RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)  # 문서를 500자씩 분할하고 50자 중첩하여 분할합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 63704개 중 10개의 문서를 로드했습니다.\n",
      "새로운 FAISS 벡터 저장소를 생성합니다.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Document' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 체인 생성\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rag \u001b[38;5;241m=\u001b[39m \u001b[43mJSONRetrievalChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[83], line 115\u001b[0m, in \u001b[0;36mRetrievalChain.create_chain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_text_splitter()  \u001b[38;5;66;03m# 텍스트 분할기를 생성합니다.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m split_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_documents(docs, text_splitter)  \u001b[38;5;66;03m# 문서를 분할합니다.\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 벡터 저장소를 생성합니다.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_retriever(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore)  \u001b[38;5;66;03m# retriever를 생성합니다.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_model()  \u001b[38;5;66;03m# 모델을 생성합니다.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[83], line 60\u001b[0m, in \u001b[0;36mRetrievalChain.create_vectorstore\u001b[0;34m(self, split_docs, cache)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m새로운 FAISS 벡터 저장소를 생성합니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m     vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcached_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     vector_store\u001b[38;5;241m.\u001b[39msave_local(FAISS_INDEX_PATH)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vector_store\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-TNmHyL-r-py3.11/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:1041\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1041\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m   1043\u001b[0m         texts,\n\u001b[1;32m   1044\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1049\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-TNmHyL-r-py3.11/lib/python3.11/site-packages/langchain/embeddings/cache.py:124\u001b[0m, in \u001b[0;36mCacheBackedEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of texts.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    The method first checks the cache for the embeddings.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m        A list of embeddings for the given texts.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     vectors: List[Union[List[\u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_embedding_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     all_missing_indices: List[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    128\u001b[0m         i \u001b[38;5;28;01mfor\u001b[39;00m i, vector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vectors) \u001b[38;5;28;01mif\u001b[39;00m vector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     ]\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m missing_indices \u001b[38;5;129;01min\u001b[39;00m batch_iterate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, all_missing_indices):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-TNmHyL-r-py3.11/lib/python3.11/site-packages/langchain/storage/encoder_backed.py:70\u001b[0m, in \u001b[0;36mEncoderBackedStore.mget\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmget\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: Sequence[K]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Optional[V]]:\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the values associated with the given keys.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     encoded_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     71\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mmget(encoded_keys)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_deserializer(value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values\n\u001b[1;32m     75\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-TNmHyL-r-py3.11/lib/python3.11/site-packages/langchain/storage/encoder_backed.py:70\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmget\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: Sequence[K]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Optional[V]]:\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the values associated with the given keys.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     encoded_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys]\n\u001b[1;32m     71\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mmget(encoded_keys)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_deserializer(value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values\n\u001b[1;32m     75\u001b[0m     ]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-TNmHyL-r-py3.11/lib/python3.11/site-packages/langchain/embeddings/cache.py:35\u001b[0m, in \u001b[0;36m_key_encoder\u001b[0;34m(key, namespace)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_key_encoder\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, namespace: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Encode a key.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43m_hash_string_to_uuid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-TNmHyL-r-py3.11/lib/python3.11/site-packages/langchain/embeddings/cache.py:29\u001b[0m, in \u001b[0;36m_hash_string_to_uuid\u001b[0;34m(input_string)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_hash_string_to_uuid\u001b[39m(input_string: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m uuid\u001b[38;5;241m.\u001b[39mUUID:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Hash a string and returns the corresponding UUID.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     hash_value \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39msha1(\u001b[43minput_string\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uuid\u001b[38;5;241m.\u001b[39muuid5(NAMESPACE_UUID, hash_value)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Document' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "# 체인 생성\n",
    "rag = JSONRetrievalChain(source_uri=\"\").create_chain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이행강제금부과처분취소에 대한 판결은 대법원에서 2013년 12월 12일 선고된 2012두20397 판결에 해당합니다. 이 판결에서는 개발제한구역의 지정 및 관리에 관한 특별조치법에 따라 이행강제금을 부과·징수할 때마다 시정명령 절차를 다시 거쳐야 하는지 여부에 대해 소극적으로 판단하였습니다. 즉, 이행강제금을 부과하기 전에 반드시 시정명령을 다시 발부할 필요는 없다는 것입니다. 또한, 이행강제금 부과의 근거가 되는 시정명령은 법률 시행일인 2010년 2월 7일 이후에 이루어져야 한다고 명시하였습니다.\n"
     ]
    }
   ],
   "source": [
    "rag_chain=rag.chain\n",
    "rag_retriever=rag.retriever\n",
    "retrieved_docs=rag_retriever.invoke(\"이행강제금부과처분취소 \")\n",
    "response = rag_chain.invoke({\n",
    "    \"context\": retrieved_docs,  # 필요한 경우 적절한 컨텍스트를 추가\n",
    "    \"question\": \"이행강제금부과처분취소에 대해 알려줘\"\n",
    "})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-TNmHyL-r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}