{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "456ce976",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc1900",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification  # 추가된 임포트\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sentencepiece as spm\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee427c",
   "metadata": {},
   "source": [
    "### Virtual Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-env",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979bbdd",
   "metadata": {},
   "source": [
    "### Load Evaluataion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-models",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/minu/Library/Caches/pypoetry/virtualenvs/langchain-kr-TNmHyL-r-py3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERT 모델 및 토크나이저 로드\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobert\", trust_remote_code=True)\n",
    "#bert_model = BertModel.from_pretrained(\"monologg/kobert\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"monologg/kobert\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f1213",
   "metadata": {},
   "source": [
    "### Load Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initialize-chatopenai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOpenAI 모델 초기화\n",
    "gpt_llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini-2024-07-18\",  # 또는 다른 OpenAI 모델\n",
    "    temperature=0.2,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")  # .env 파일에서 API 키 로드\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c285d9",
   "metadata": {},
   "source": [
    "### RAG Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52e94f",
   "metadata": {},
   "source": [
    "Embedding : UpstageEmbeddings\n",
    "text_splitter : RecursiveCharacterTextSplitter\n",
    "vectorstore : pinecone,faiss\n",
    "document_loader : PDFPlumberLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ[\"OPENAI_API_KEY\"])\n",
    "print(os.environ[\"PINECONE_API_KEY\"])\n",
    "print(os.environ[\"UPSTAGE_API_KEY\"])\n",
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369a59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_teddynote import logging\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH09-VectorStores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_teddynote.korean import stopwords\n",
    "\n",
    "# 한글 불용어 사전 불러오기 (불용어 사전 출처: https://www.ranks.nl/stopwords/korean)\n",
    "stopword = stopwords()\n",
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "setup-embeddings-vectorstore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 중인 파일: Minbub Selected Provisions.pdf\n",
      "처리 중인 파일: Criminal Law Selected Provisions.pdf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m             documents\u001b[38;5;241m.\u001b[39mextend(text_splitter\u001b[38;5;241m.\u001b[39msplit_text(doc\u001b[38;5;241m.\u001b[39mpage_content))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m---> 23\u001b[0m     documents\u001b[38;5;241m.\u001b[39mextend(\u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_texts(documents, passage_embeddings)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-TNmHyL-r-py3.11/lib/python3.11/site-packages/langchain_text_splitters/character.py:118\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter.split_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_separators\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-TNmHyL-r-py3.11/lib/python3.11/site-packages/langchain_text_splitters/character.py:94\u001b[0m, in \u001b[0;36mRecursiveCharacterTextSplitter._split_text\u001b[0;34m(self, text, separators)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     93\u001b[0m _separator \u001b[38;5;241m=\u001b[39m separator \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_separator_regex \u001b[38;5;28;01melse\u001b[39;00m re\u001b[38;5;241m.\u001b[39mescape(separator)\n\u001b[0;32m---> 94\u001b[0m splits \u001b[38;5;241m=\u001b[39m \u001b[43m_split_text_with_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_separator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keep_separator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Now go merging things, recursively splitting longer texts.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m _good_splits \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-TNmHyL-r-py3.11/lib/python3.11/site-packages/langchain_text_splitters/character.py:38\u001b[0m, in \u001b[0;36m_split_text_with_regex\u001b[0;34m(text, separator, keep_separator)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m separator:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keep_separator:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# The parentheses in the pattern keep the delimiters in the result.\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m         _splits \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseparator\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m         splits \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     40\u001b[0m             ([_splits[i] \u001b[38;5;241m+\u001b[39m _splits[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(_splits) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)])\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m keep_separator \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m ([_splits[i] \u001b[38;5;241m+\u001b[39m _splits[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(_splits), \u001b[38;5;241m2\u001b[39m)])\n\u001b[1;32m     43\u001b[0m         )\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_splits) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.10/lib/python3.11/re/__init__.py:206\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(pattern, string, maxsplit, flags)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(pattern, string, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Split the source string by the occurrences of the pattern,\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    returning a list containing the resulting substrings.  If\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    capturing parentheses are used in pattern, then the text of all\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    and the remainder of the string is returned as the final element\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    of the list.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msplit(string, maxsplit)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 임베딩 및 벡터스토어 설정\n",
    "passage_embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "DATA_DIR_PATH = \"./data\"\n",
    "documents= []\n",
    "\n",
    "# data 디렉토리 내의 모든 PDF 파일 처리\n",
    "for filename in os.listdir(DATA_DIR_PATH):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(DATA_DIR_PATH, filename)\n",
    "        print(f\"처리 중인 파일: {filename}\")\n",
    "        \n",
    "        # PDF 문서 로더 인스턴스 생성 및 문서 로딩\n",
    "        loader = PDFPlumberLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        # 문서를 청크로 분할하고 all_texts에 추가\n",
    "        for doc in docs:\n",
    "            documents.extend(text_splitter.split_text(doc.page_content))\n",
    "\n",
    "for doc in documents:\n",
    "    documents.extend(text_splitter.split_text(doc))\n",
    "vectorstore = FAISS.from_texts(documents, passage_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4adbf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 2개의 PDF 파일을 처리합니다.\n",
      "처리 중인 파일 (1/2): Minbub Selected Provisions.pdf\n",
      "처리 중인 파일 (2/2): Criminal Law Selected Provisions.pdf\n",
      "모든 PDF 파일 처리 완료. 총 3047개의 텍스트 청크가 생성되었습니다.\n",
      "벡터 저장소 생성 중...\n",
      "벡터 저장소 생성 완료.\n",
      "\n",
      "작업 완료 항목:\n",
      "1. 2개의 PDF 파일 처리\n",
      "2. 3047개의 텍스트 청크 생성\n",
      "3. FAISS 벡터 저장소 생성\n",
      "\n",
      "총 소요 시간: 455.51초\n",
      "\n",
      "처리된 텍스트의 일부 내용 예시:\n",
      "민법\n",
      "민법\n",
      "[시행 2024.5.17.][법률 제19409호, 2023.5.16., 타법개정]\n",
      "제1편 총칙\n",
      "제1장 통칙\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 임베딩 및 벡터스토어 설정\n",
    "passage_embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "DATA_DIR_PATH = \"./data\"\n",
    "documents = []\n",
    "\n",
    "# 전체 PDF 파일 수 계산\n",
    "total_files = sum(1 for filename in os.listdir(DATA_DIR_PATH) if filename.endswith(\".pdf\"))\n",
    "processed_files = 0\n",
    "\n",
    "print(f\"총 {total_files}개의 PDF 파일을 처리합니다.\")\n",
    "\n",
    "# data 디렉토리 내의 모든 PDF 파일 처리\n",
    "for filename in os.listdir(DATA_DIR_PATH):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(DATA_DIR_PATH, filename)\n",
    "        processed_files += 1\n",
    "        print(f\"처리 중인 파일 ({processed_files}/{total_files}): {filename}\")\n",
    "        \n",
    "        # PDF 문서 로더 인스턴스 생성 및 문서 로딩\n",
    "        loader = PDFPlumberLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        # 문서를 청크로 분할하고 documents에 추가\n",
    "        for doc in docs:\n",
    "            documents.extend(text_splitter.split_text(doc.page_content))\n",
    "\n",
    "print(f\"모든 PDF 파일 처리 완료. 총 {len(documents)}개의 텍스트 청크가 생성되었습니다.\")\n",
    "\n",
    "# 벡터 저장소 생성\n",
    "print(\"벡터 저장소 생성 중...\")\n",
    "vectorstore = FAISS.from_texts(documents, passage_embeddings)\n",
    "print(\"벡터 저장소 생성 완료.\")\n",
    "\n",
    "# 종료 시간 기록 및 총 소요 시간 계산\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n작업 완료 항목:\")\n",
    "print(f\"1. {total_files}개의 PDF 파일 처리\")\n",
    "print(f\"2. {len(documents)}개의 텍스트 청크 생성\")\n",
    "print(f\"3. FAISS 벡터 저장소 생성\")\n",
    "print(f\"\\n총 소요 시간: {total_time:.2f}초\")\n",
    "\n",
    "# 처리된 텍스트의 일부 내용 출력 (선택사항)\n",
    "if documents:\n",
    "    print(\"\\n처리된 텍스트의 일부 내용 예시:\")\n",
    "    print(documents[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710cd66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51b9e72b",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "define-agentstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 정의\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[HumanMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12757714",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "define-evaluate-question",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가 단계 실행 시간: 0.19초\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='평가 결과: 일반 법률 질문')]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 노드 함수들\n",
    "def evaluate_question(state):\n",
    "    start_time = time.time()\n",
    "    messages = state['messages']\n",
    "    question = messages[-1].content\n",
    "    inputs = bert_tokenizer(question, return_tensors='pt')\n",
    "    outputs = bert_model(**inputs)\n",
    "    classification = torch.argmax(outputs.logits, dim=1).item()\n",
    "    classification_map = {0: \"일반 법률 질문\", 1: \"복잡한 법률 질문\"}\n",
    "    evaluation = classification_map.get(classification, \"알 수 없음\")\n",
    "    end_time = time.time()\n",
    "    print(f\"평가 단계 실행 시간: {end_time - start_time:.2f}초\")\n",
    "    return {\"messages\": [HumanMessage(content=f\"평가 결과: {evaluation}\")]}\n",
    "\n",
    "# 테스트 코드\n",
    "state = {'messages': [HumanMessage(content='계약 위반 시 손해배상 청구는 어떻게 하나요?')]}\n",
    "evaluate_question(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "define-retrieve-info",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 단계 실행 시간: 0.60초\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='검색된 정보: 계약의 위반 시 손해배상이 청구될 수 있습니다.\\n계약은 양 당사자 간의 합의에 의해 성립됩니다.\\n계약서에 명시된 조항은 법적 구속력을 가집니다.')]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve_info(state):\n",
    "    start_time = time.time()\n",
    "    messages = state['messages']\n",
    "    query = messages[-1].content + \" \" + messages[0].content\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    retrieved_info = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    end_time = time.time()\n",
    "    print(f\"검색 단계 실행 시간: {end_time - start_time:.2f}초\")\n",
    "    return {\"messages\": [HumanMessage(content=f\"검색된 정보: {retrieved_info}\")]}\n",
    "\n",
    "# 테스트 코드\n",
    "state = {'messages': [HumanMessage(content='계약 위반 시 손해배상 청구는 어떻게 하나요?'), HumanMessage(content='평가 결과: 일반 법률 질문')]}\n",
    "retrieve_info(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "define-generate-response",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성 단계 실행 시간: 4.93초\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='AI 응답: 계약 위반 시 손해배상 청구를 진행하는 방법은 다음과 같습니다:\\n\\n1. **계약서 검토**: 우선, 계약서의 내용을 자세히 검토하여 위반된 조항이 무엇인지 확인합니다. 계약서에는 위반 시의 손해배상에 대한 조항이 포함되어 있을 수 있으므로 이를 참고하는 것이 중요합니다.\\n\\n2. **위반 사실 확인**: 상대방이 계약을 위반했음을 입증할 수 있는 증거를 수집합니다. 이메일, 문자 메시지, 계약서 사본 등 관련 자료를 확보하는 것이 좋습니다.\\n\\n3. **손해액 산정**: 계약 위반으로 인해 발생한 손해를 구체적으로 산정합니다. 손해는 직접적인 금전적 손실뿐만 아니라, 계약 이행으로 기대할 수 있었던 이익도 포함될 수 있습니다.\\n\\n4. **상대방에게 통지**: 계약 위반 사실과 손해배상 청구 의사를 상대방에게 공식적으로 통지합니다. 이때 내용증명 우편을 이용하면 증거로 활용할 수 있습니다.\\n\\n5. **협상 시도**: 상대방과의 협상을 통해 손해배상에 대한 합의를 시도할 수 있습니다. 이 과정에서 법률 전문가의 도움을 받는 것도 유익합니다.\\n\\n6. **소송 제기**: 협상이 실패할 경우, 법원에 손해배상 청구 소송을 제기할 수 있습니다. 이때는 법률 전문가의 조력을 받는 것이 좋습니다. 소송을 통해 법원의 판결을 받아 손해배상을 청구할 수 있습니다.\\n\\n7. **소송 절차**: 소송이 진행되면, 법원에서의 절차에 따라 증거 제출, 변론 등을 진행하게 됩니다. 최종적으로 법원에서 판결이 내려지면, 상대방에게 손해배상을 청구할 수 있습니다.\\n\\n계약 위반에 대한 손해배상 청구는 복잡할 수 있으므로, 법률 전문가와 상담하여 구체적인 상황에 맞는 조언을 받는 것이 중요합니다.')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_response(state):\n",
    "    start_time = time.time()\n",
    "    messages = state['messages']\n",
    "    evaluation = messages[1].content\n",
    "    retrieved_info = messages[2].content\n",
    "    user_input = messages[0].content\n",
    "    \n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"evaluation\", \"retrieved_info\", \"user_input\"],\n",
    "        template=\"\"\"\n",
    "        당신은 전문 법률 AI 어시스턴트입니다.\n",
    "        \n",
    "        {evaluation}\n",
    "        {retrieved_info}\n",
    "        \n",
    "        사용자 질문: {user_input}\n",
    "        \n",
    "        위의 평가 결과와 검색된 정보를 바탕으로 사용자에게 도움이 되는 법률 답변을 제공하세요.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    prompt = prompt_template.format(\n",
    "        evaluation=evaluation,\n",
    "        retrieved_info=retrieved_info,\n",
    "        user_input=user_input\n",
    "    )\n",
    "    response = gpt_llm.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    print(f\"생성 단계 실행 시간: {end_time - start_time:.2f}초\")\n",
    "    return {\"messages\": [HumanMessage(content=f\"AI 응답: {response.content}\")]}\n",
    "\n",
    "# 테스트 코드\n",
    "state = {'messages': [\n",
    "    HumanMessage(content='계약 위반 시 손해배상 청구는 어떻게 하나요?'),\n",
    "    HumanMessage(content='평가 결과: 일반 법률 질문'),\n",
    "    HumanMessage(content='검색된 정보: 계약은 양 당사자 간의 합의에 의해 성립됩니다.')\n",
    "]}\n",
    "generate_response(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef559073",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "create-graph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompiledStateGraph(nodes={'__start__': PregelNode(config={'tags': ['langsmith:hidden'], 'metadata': {}, 'configurable': {}}, channels=['__start__'], triggers=['__start__'], writers=[ChannelWrite<messages>(recurse=True, writes=[ChannelWriteEntry(channel='messages', value=<object object at 0x16a9df0f0>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages']), ChannelWrite<start:평가>(recurse=True, writes=[ChannelWriteEntry(channel='start:평가', value='__start__', skip_none=False, mapper=None)], require_at_least_one_of=None)]), '평가': PregelNode(config={'tags': [], 'metadata': {}, 'configurable': {}}, channels={'messages': 'messages'}, triggers=['start:평가'], mapper=functools.partial(<function _coerce_state at 0x17e5911c0>, <class '__main__.AgentState'>), writers=[ChannelWrite<평가,messages>(recurse=True, writes=[ChannelWriteEntry(channel='평가', value='평가', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x16a9df0f0>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages'])]), '검색': PregelNode(config={'tags': [], 'metadata': {}, 'configurable': {}}, channels={'messages': 'messages'}, triggers=['평가'], mapper=functools.partial(<function _coerce_state at 0x17e5911c0>, <class '__main__.AgentState'>), writers=[ChannelWrite<검색,messages>(recurse=True, writes=[ChannelWriteEntry(channel='검색', value='검색', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x16a9df0f0>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages'])]), '생성': PregelNode(config={'tags': [], 'metadata': {}, 'configurable': {}}, channels={'messages': 'messages'}, triggers=['검색'], mapper=functools.partial(<function _coerce_state at 0x17e5911c0>, <class '__main__.AgentState'>), writers=[ChannelWrite<생성,messages>(recurse=True, writes=[ChannelWriteEntry(channel='생성', value='생성', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x16a9df0f0>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages'])])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x3336afb50>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x3336aca90>, '평가': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x3336ac510>, '검색': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x17eebd010>, '생성': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x334c466d0>, 'start:평가': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x334c459d0>}, auto_validate=False, stream_mode='updates', output_channels=['messages'], stream_channels=['messages'], input_channels='__start__', builder=<langgraph.graph.state.StateGraph object at 0x330906810>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그래프 생성\n",
    "def create_graph():\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    workflow.add_node(\"평가\", evaluate_question)\n",
    "    workflow.add_node(\"검색\", retrieve_info)\n",
    "    workflow.add_node(\"생성\", generate_response)\n",
    "    \n",
    "    workflow.set_entry_point(\"평가\")\n",
    "    workflow.add_edge(\"평가\", \"검색\")\n",
    "    workflow.add_edge(\"검색\", \"생성\")\n",
    "    workflow.add_edge(\"생성\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# 테스트 코드\n",
    "create_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838c4f6",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "run-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가 단계 실행 시간: 0.06초\n",
      "검색 단계 실행 시간: 0.49초\n",
      "생성 단계 실행 시간: 4.55초\n",
      "AI 응답: 계약 위반 시 손해배상을 청구하는 과정은 다음과 같은 단계로 진행됩니다:\n",
      "\n",
      "1. **계약서 검토**: 먼저, 해당 계약서를 자세히 검토하여 위반된 조항이 무엇인지 확인합니다. 계약서에 명시된 조항은 법적 구속력을 가지므로, 위반 사항이 계약서에 명시되어 있어야 합니다.\n",
      "\n",
      "2. **손해 발생 확인**: 계약 위반으로 인해 발생한 손해를 확인합니다. 손해는 직접적인 금전적 손실뿐만 아니라, 계약 이행으로 인해 기대할 수 있었던 이익의 상실 등도 포함될 수 있습니다.\n",
      "\n",
      "3. **상대방에게 통지**: 계약 위반 사실을 상대방에게 통지합니다. 이때, 위반 사항과 손해를 구체적으로 설명하고, 손해배상을 요구하는 내용을 포함해야 합니다. 통지서는 서면으로 작성하는 것이 좋습니다.\n",
      "\n",
      "4. **협상 시도**: 상대방과의 협상을 통해 손해배상에 대한 합의를 시도할 수 있습니다. 이 과정에서 상대방이 손해배상에 응할 경우, 합의서를 작성하여 양 당사자가 서명하는 것이 중요합니다.\n",
      "\n",
      "5. **법적 절차 진행**: 만약 협상이 실패할 경우, 법적 절차를 진행할 수 있습니다. 이 경우, 손해배상 청구 소송을 제기해야 하며, 이를 위해 변호사의 도움을 받는 것이 좋습니다. 소송을 제기할 때는 계약서, 손해 발생에 대한 증거, 통지서 등의 자료를 준비해야 합니다.\n",
      "\n",
      "6. **법원 판결**: 법원에서 사건을 심리한 후 판결이 내려지면, 그에 따라 손해배상을 받을 수 있습니다.\n",
      "\n",
      "계약 위반에 대한 손해배상 청구는 복잡할 수 있으므로, 법률 전문가와 상담하는 것이 좋습니다. 이를 통해 보다 정확하고 효과적인 대응을 할 수 있습니다.\n",
      "전체 실행 시간: 5.13초\n"
     ]
    }
   ],
   "source": [
    "# 실행 함수\n",
    "def run_pipeline(user_input):\n",
    "    start_time = time.time()\n",
    "    graph = create_graph()\n",
    "    inputs = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "    for output in graph.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            if key == \"생성\":\n",
    "                print(value['messages'][-1].content)\n",
    "    end_time = time.time()\n",
    "    print(f\"전체 실행 시간: {end_time - start_time:.2f}초\")\n",
    "\n",
    "# 실행 예시\n",
    "user_input = '계약 위반 시 손해배상 청구는 어떻게 하나요?'\n",
    "run_pipeline(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
