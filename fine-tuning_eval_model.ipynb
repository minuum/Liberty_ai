{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bnb_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#@param {type: \"string\"}\u001b[39;00m\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#@param {type: \"string\"}\u001b[39;00m\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     31\u001b[0m model,\n\u001b[0;32m---> 32\u001b[0m quantization_config\u001b[38;5;241m=\u001b[39m\u001b[43mbnb_config\u001b[49m,\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#load_in_4bit=True, # Quantization Load\u001b[39;00m\n\u001b[1;32m     34\u001b[0m device_map\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     36\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_LLM_model)\n\u001b[1;32m     38\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m안녕하세요.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bnb_config' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import fire\n",
    "import json\n",
    "from typing import List, Union\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import transformers\n",
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl, BitsAndBytesConfig\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    set_peft_model_state_dict\n",
    ")\n",
    "from peft import PeftModel\n",
    "import re\n",
    "import ast\n",
    "\n",
    "device = 'auto' #@param {type: \"string\"}\n",
    "model = 'AIDX-ktds/ktdsbaseLM-v0.12-based-on-openchat3.5' #@param {type: \"string\"}\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "model,\n",
    "quantization_config=bnb_config,\n",
    "#load_in_4bit=True, # Quantization Load\n",
    "device_map=device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_LLM_model)\n",
    "\n",
    "input_text = \"안녕하세요.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "inputs = inputs.to(\"cuda:0\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_length=1024)\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-TNmHyL-r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
