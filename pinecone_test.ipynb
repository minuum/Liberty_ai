{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PineconeRetrievalChain 테스트 노트북"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if PINECONE_API_KEY:\n",
    "    print(\"Pinecone API Key가 제대로 로드되었습니다.\")\n",
    "else:\n",
    "    print(\"Pinecone API Key가 로드되지 않았습니다. .env 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Pinecone 인덱스 생성 및 초기화\n",
    "import pinecone_ as pinecone\n",
    "\n",
    "# 환경 변수 로드 (dotenv 파일 설정이 되어 있어야 합니다)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Pinecone API Key 및 인덱스 이름\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\", \"liberty-rag-json\")\n",
    "\n",
    "# Pinecone 인덱스 초기화\n",
    "retrieval_chain = pinecone.PineconeRetrievalChain(index_name=PINECONE_INDEX_NAME)\n",
    "print(f\"Pinecone 인덱스가 성공적으로 초기화되었습니다: {retrieval_chain.pinecone_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone Index 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Index, init\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME=os.getenv(\"PINECONE_INDEX_NAME\", \"liberty-ai-index\")\n",
    "def create_index(api_key: str=PINECONE_API_KEY, index_name: str=PINECONE_INDEX_NAME, dimension: int = 768, metric: str = \"dotproduct\", host=\"https://liberty-rag-json-hwsbh8f.svc.aped-4627-b74a.pinecone.io\"):\n",
    "    \"\"\"Pinecone 인덱스를 생성하고 반환합니다.\"\"\"\n",
    "    # Pinecone 클라이언트를 API 키로 초기화\n",
    "    #init(api_key=api_key,environment=PINECONE_ENVIRONMENT)\n",
    "    \n",
    "    try:\n",
    "            # 이미 존재하는 인덱스를 가져옴\n",
    "        index = Index(index_name, host=host)\n",
    "        print(f\"인덱스 '{index_name}'가 이미 존재합니다.\")\n",
    "    except Exception as e:\n",
    "        # 인덱스가 존재하지 않으면 새로 생성\n",
    "        print(f\"인덱스 '{index_name}'가 존재하지 않아서 새로 생성합니다.\")\n",
    "        from pinecone import create_index\n",
    "        create_index(\n",
    "                name=index_name,\n",
    "                dimension=dimension,  # 임베딩 차원수 (모델에 맞게 설정)\n",
    "                metric=metric\n",
    "            )\n",
    "        index = Index(index_name, host=host)\n",
    "        print(f\"새로운 인덱스 '{index_name}' 생성 완료\")\n",
    "        \n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #\"\"\"Pinecone 인덱스를 초기화하거나 존재하지 않으면 생성합니다.\"\"\"\n",
    "# host = \"https://liberty-index-hwsbh8f.svc.aped-4627-b74a.pinecone.io\"\n",
    "# index = create_index(api_key=PINECONE_API_KEY, host=host, index_name=PINECONE_INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Pinecone 클라이언트 초기화\n",
    "host = \"https://liberty-rag-json-hwsbh8f.svc.aped-4627-b74a.pinecone.io\"\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# 인덱스 접근\n",
    "index = pc.Index(host=host)  # describe_index()로 host 확인 가능\n",
    "pinecone_params = {\"index\": index, \"namespace\": PINECONE_INDEX_NAME + \"-namespace-01\"}\n",
    "\n",
    "# 인덱스와 pinecone_params 테스트\n",
    "print(f\"인덱스 이름: {index.describe_index_stats()}\")\n",
    "print(f\"네임스페이스: {pinecone_params['namespace']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 업로드(json loading, split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: 문서 로드 및 전처리 테스트\n",
    "import pinecone_\n",
    "import os\n",
    "\n",
    "# 데이터 경로 설정\n",
    "data_dir = \"data/154.의료, 법률 전문 서적 말뭉치/01-1.정식개방데이터/Training/02.라벨링데이터/Training_legal.json\"  # data 폴더에 PDF 파일들을 저장해 주세요\n",
    "import json\n",
    "split_docs=[]\n",
    "with open(data_dir, 'r', encoding='utf-8') as f:\n",
    "    try:\n",
    "        json_data = json.load(f)\n",
    "                    \n",
    "        # data 배열에서 문서 추출 (처음 10개만)\n",
    "        if isinstance(json_data, dict) and 'data' in json_data:\n",
    "            for item in json_data['data']:  # 처음 10개의 문서만 로드\n",
    "                if isinstance(item, dict):\n",
    "                    # Document 객체 생성\n",
    "                    from langchain.schema import Document\n",
    "                    doc = Document(\n",
    "                        page_content=item.get('text', ''),\n",
    "                        metadata={\n",
    "                            'book_id': item.get('book_id'),\n",
    "                            'category': item.get('category'),\n",
    "                            'popularity': item.get('popularity'),\n",
    "                            'keyword': item.get('keyword', []),\n",
    "                            'word_segment': item.get('word_segment', []),\n",
    "                            'publication_ymd': item.get('publication_ymd')\n",
    "                        }\n",
    "                    )\n",
    "                    print(\"출력 중\")\n",
    "                    split_docs.append(doc)\n",
    "                    print(\"출력 완료\")\n",
    "                else:\n",
    "                    print(\"JSON 데이터가 예상된 형식이 아닙니다.\")\n",
    "                    print(\"데이터 구조:\", json_data.keys() if isinstance(json_data, dict) else type(json_data))\n",
    "            \n",
    "            print(f\"전체 {len(json_data['data'])}개 중 {len(split_docs)}개의 문서를 로드했습니다.\")\n",
    "                        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON 파일 파싱 중 오류가 발생했습니다: {e}\")\n",
    "print(split_docs[0].metadata)\n",
    "print(split_docs[0].page_content)\n",
    "print(split_docs[0].metadata['book_id'])\n",
    "print(split_docs[0].metadata['category'])\n",
    "print(split_docs[0].metadata['popularity'])\n",
    "print(split_docs[0].metadata['keyword'])\n",
    "print(split_docs[0].metadata['word_segment'])\n",
    "print(split_docs[0].metadata['publication_ymd'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "from tqdm import tqdm\n",
    "def preprocess_documents(\n",
    "    split_docs: List[Any], \n",
    "    metadata_keys: List[str] = [\"book_id\", \"category\", \"popularity\", \"keyword\", \"word_segment\", \"publication_ymd\"], \n",
    "    min_length: int = 5\n",
    "):\n",
    "    \"\"\"문서를 전처리하고 내용과 메타데이터를 반환합니다.\"\"\"\n",
    "    contents = []\n",
    "    metadatas = {key: [] for key in metadata_keys}\n",
    "    \n",
    "    for doc in tqdm(split_docs, desc=\"문서 전처리 중\"):\n",
    "        content = doc.page_content.strip()\n",
    "        if content and len(content) >= min_length:\n",
    "            # 컨텍스트 길이 제한 (Pinecone 권장사항)\n",
    "            contents.append(content[:4000])  \n",
    "            \n",
    "            # 메타데이터 추출\n",
    "            for k in metadata_keys:\n",
    "                value = doc.metadata.get(k)\n",
    "                metadatas[k].append(value)\n",
    "                \n",
    "    print(f\"전체 {len(split_docs)}개 중 {len(contents)}개의 문서가 전처리되었습니다.\")\n",
    "    return contents, metadatas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4000자 넘는 문서 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import KonlpyTextSplitter\n",
    "\n",
    "model_name=\"gpt-4o-mini-2024-07-18\"\n",
    "\n",
    "#split_texts = texts_tik.split_text(contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "from tqdm import tqdm\n",
    "def preprocess_documents(\n",
    "    split_docs: List[Any], \n",
    "    metadata_keys: List[str] = [\"book_id\", \"category\", \"popularity\", \"keyword\", \"word_segment\", \"publication_ymd\"], \n",
    "    min_length: int = 5\n",
    "):\n",
    "    \"\"\"문서를 전처리하고 내용과 메타데이터를 반환합니다.\"\"\"\n",
    "    contents = []\n",
    "    metadatas = {key: [] for key in metadata_keys}\n",
    "    \n",
    "    for doc in tqdm(split_docs[:20], desc=\"문서 전처리 중\"):\n",
    "        content = doc.page_content.strip()\n",
    "        if content and len(content) >= min_length:\n",
    "            # 컨텍스트가 4000자를 넘는 경우 분할\n",
    "            if len(content) > 4000:\n",
    "                print(\"분할\", doc.metadata['book_id'], len(content))\n",
    "                # 청크 수 계산\n",
    "                num_chunks = (len(content) // 4000) + 1\n",
    "                chunk_size = len(content) // num_chunks\n",
    "                chunk_overlap = 20\n",
    "                text_splitter = KonlpyTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "                #texts_tik = text_splitter.from_tiktoken_encoder(model_name=model_name, chunk_size=chunk_size, chunk_overlap=chunk_overlap)  \n",
    "                #split_chunks = texts_tik.split_text(content)\n",
    "                split_chunks = text_splitter.split_text(content)\n",
    "                \n",
    "                for chunk in split_chunks:\n",
    "                    contents.append(chunk.strip())\n",
    "                    print(\"청크\", doc.metadata['book_id'], num_chunks, len(chunk))\n",
    "                    # 메타데이터에 청크 번호 추가하여 복사\n",
    "                    metadata_values = {k: doc.metadata.get(k) for k in metadata_keys}\n",
    "                    metadata_values[\"book_id\"] = f\"{metadata_values['book_id']}_{num_chunks}\"\n",
    "                    for k in metadata_keys:\n",
    "                        metadatas[k].append(metadata_values[k])\n",
    "                   \n",
    "            else:\n",
    "                # 4000자 이하인 경우 그대로 저장\n",
    "                contents.append(content)\n",
    "                for k in metadata_keys:\n",
    "                    value = doc.metadata.get(k)\n",
    "                    metadatas[k].append(value)\n",
    "                \n",
    "    print(f\"전체 {len(split_docs)}개 중 {len(contents)}개의 문서가 전처리되었습니다.\")\n",
    "    return contents, metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents, metadatas = preprocess_documents(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min(10,len(contents))):\n",
    "    print(f\"문서 내용: {contents[i]}\")\n",
    "    print(f\"책 ID: {metadatas['book_id'][i]}\")\n",
    "    print(f\"문서 길이: {len(contents[i])}자\")\n",
    "    print(\"-\" * 40)  # 분할 여부를 확인할 수 있는 구분선\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_teddynote.korean import stopwords\n",
    "\n",
    "# 한글 불용어 사전 불러오기 (불용어 사전 출처: https://www.ranks.nl/stopwords/korean)\n",
    "stopword = stopwords()\n",
    "#stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.community.pinecone import (\n",
    "    create_sparse_encoder,\n",
    "    fit_sparse_encoder,\n",
    ")\n",
    "\n",
    "# 한글 불용어 사전 + Kiwi 형태소 분석기를 사용합니다.\n",
    "sparse_encoder = create_sparse_encoder(stopwords(), mode=\"kiwi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Encoder 를 사용하여 contents 를 학습\n",
    "saved_path = fit_sparse_encoder(\n",
    "    sparse_encoder=sparse_encoder, contents=contents, save_path=\"./sparse_encoder.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.community.pinecone import load_sparse_encoder\n",
    "\n",
    "# 추후에 학습된 sparse encoder 를 불러올 때 사용합니다.\n",
    "sparse_encoder = load_sparse_encoder(\"./sparse_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_teddynote.community.kiwi_tokenizer import KiwiBM25Tokenizer\n",
    "passage_embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large-query\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Sparse Encoder 학습 및 로드 \n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import secrets\n",
    "import itertools\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Index, init, Pinecone\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from typing import List, Dict, Any\n",
    "from langchain_teddynote.community.kiwi_tokenizer import KiwiBM25Tokenizer\n",
    "def generate_hash() -> str:\n",
    "    \"\"\"24자리 무작위 hex 값을 생성하고 6자리씩 나누어 '-'로 연결합니다.\"\"\"\n",
    "    random_hex = secrets.token_hex(12)\n",
    "    return \"-\".join(random_hex[i: i + 6] for i in range(0, 24, 6))\n",
    "\n",
    "from pinecone import Index, init\n",
    "import itertools\n",
    "def chunks(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    chunk = list(itertools.islice(it, size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = list(itertools.islice(it, size))\n",
    "\n",
    "# def upsert_documents_parallel(contents, metadatas, sparse_encoder, pinecone_params, embedder=UpstageEmbeddings(model=\"solar-embedding-1-large-query\"), batch_size=100, max_workers=30):\n",
    "#     # 문자열을 Document 객체로 변환\n",
    "from langchain.schema import Document\n",
    "keys = list(metadatas.keys())\n",
    "embedder=passage_embeddings\n",
    "    \n",
    "    # documents = [\n",
    "    #     Document(\n",
    "    #         page_content=text,\n",
    "    #         metadata={\n",
    "    #             'book_id': metadatas['book_id'][i],\n",
    "    #             'category': metadatas['category'][i], \n",
    "    #             'popularity': metadatas['popularity'][i],\n",
    "    #             'keyword': metadatas['keyword'][i],\n",
    "    #             'word_segment': metadatas['word_segment'][i],\n",
    "    #             'publication_ymd': metadatas['publication_ymd'][i]\n",
    "    #         }\n",
    "    #     ) for i, text in enumerate(batch)\n",
    "    # ]\n",
    "\n",
    "def process_batch(batch, contents, metadatas, sparse_encoder, passage_embeddings, pinecone_params):\n",
    "    \"\"\"\n",
    "    배치 단위로 문서를 처리하여 Pinecone에 업로드합니다.\n",
    "    \n",
    "    Args:\n",
    "        batch: 현재 처리할 문서들의 인덱스 리스트\n",
    "        contents: 전체 문서 내용 리스트\n",
    "        metadatas: 전체 문서의 메타데이터 딕셔너리\n",
    "        sparse_encoder: 희소 임베딩을 생성하는 인코더\n",
    "        passage_embeddings: 밀집 임베딩을 생성하는 인코더\n",
    "        pinecone_params: Pinecone 관련 설정\n",
    "    \"\"\"\n",
    "    # 현재 배치에 해당하는 문서와 메타데이터 추출\n",
    "    context_batch = [contents[i] for i in batch]\n",
    "    metadata_keys = ['book_id', 'category', 'popularity', 'keyword', 'word_segment', 'publication_ymd']\n",
    "    \n",
    "    batch_result = [\n",
    "        {\n",
    "            \"context\": context[:1000],  # 컨텍스트 길이 제한\n",
    "            **{key: metadatas[key][i] for key in metadata_keys}\n",
    "        } for i, context in enumerate(context_batch)\n",
    "    ]\n",
    "\n",
    "    ids = [generate_hash() for _ in range(len(batch))]\n",
    "    dense_embeds = passage_embeddings.embed_documents(context_batch)\n",
    "    sparse_embeds = sparse_encoder.encode_documents(context_batch)\n",
    "    print(f\"Processing batch: {batch}\")\n",
    "    vectors = [\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"values\": dense_embed,\n",
    "            \"sparse_values\": sparse_embed,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        for id_, dense_embed, sparse_embed, metadata in zip(\n",
    "            ids, dense_embeds, sparse_embeds, batch_result\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        return index.upsert(\n",
    "            vectors=vectors,\n",
    "            namespace=pinecone_params[\"namespace\"],\n",
    "            async_req=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Upsert 중 오류 발생: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "max_workers=30\n",
    "# 57,005번째 문서부터 처리\n",
    "start_idx = 57005\n",
    "contents_subset = contents[start_idx:]\n",
    "metadata_subset = {key: values[start_idx:] for key, values in metadatas.items()}\n",
    "\n",
    "batches = list(chunks(range(len(contents_subset)), batch_size))\n",
    "print(batches)\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            process_batch, \n",
    "            batch,\n",
    "            contents_subset,\n",
    "            metadata_subset,\n",
    "            sparse_encoder,\n",
    "            passage_embeddings,\n",
    "            pinecone_params\n",
    "        ) for batch in batches\n",
    "    ]\n",
    "    print(futures)\n",
    "    results = []\n",
    "    \n",
    "    # tqdm 설정 추가\n",
    "    total_docs = len(contents_subset)\n",
    "    pbar = tqdm(total=total_docs, desc=\"문서 Upsert 중\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    processed_docs = 0\n",
    "    for idx, future in enumerate(as_completed(futures)):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            \n",
    "        # 진행률과 예상 시간 계산\n",
    "        elapsed_time = time.time() - start_time\n",
    "        processed_docs += len(batches[idx])\n",
    "        progress = processed_docs / total_docs\n",
    "        estimated_total_time = elapsed_time / progress if progress > 0 else 0\n",
    "        remaining_time = estimated_total_time - elapsed_time\n",
    "        \n",
    "        # 진행 상태 업데이트\n",
    "        pbar.set_postfix({\n",
    "            'progress': f'{processed_docs}/{total_docs}',\n",
    "            'remaining': f'{remaining_time:.1f}s'\n",
    "        })\n",
    "        pbar.update(len(batches[idx]))\n",
    "    \n",
    "    pbar.close()\n",
    "    total_upserted = sum(result.upserted_count for result in results if result)\n",
    "    print(f\"총 {total_upserted}개의 Vector가 Upsert 되었습니다.\")\n",
    "    print(f\"{pinecone_params['index'].describe_index_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 57,005번째 문서부터 처리하기 위한 데이터 슬라이싱\n",
    "start_idx = min(63004, len(contents))\n",
    "contents_subset = contents[start_idx:]\n",
    "metadata_subset = {key: values[start_idx:] for key, values in metadatas.items()}\n",
    "print(f\"Contents subset length: {len(contents_subset)}\")\n",
    "print(f\"Metadata subset length: {len(metadata_subset)}\")\n",
    "batch_size = 100\n",
    "max_workers = 30\n",
    "\n",
    "# 배치 생성\n",
    "batches = list(chunks(range(len(contents_subset)), batch_size))\n",
    "print(f\"Number of batches: {len(batches)}\")\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            process_batch, \n",
    "            batch,\n",
    "            contents_subset,  # 슬라이싱된 contents 사용\n",
    "            metadata_subset,  # 슬라이싱된 metadata 사용\n",
    "            sparse_encoder,\n",
    "            passage_embeddings,\n",
    "            pinecone_params\n",
    "        ) for batch in batches\n",
    "    ]\n",
    "    print(f\"Number of futures: {len(futures)}\")\n",
    "    # 진행 상황 모니터링\n",
    "    total_docs = len(contents_subset)\n",
    "    pbar = tqdm(total=total_docs, desc=\"문서 Upsert 중\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = []\n",
    "    processed_docs = 0\n",
    "    \n",
    "    for idx, future in enumerate(as_completed(futures)):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            \n",
    "        processed_docs += len(batches[idx])\n",
    "        elapsed_time = time.time() - start_time\n",
    "        progress = processed_docs / total_docs\n",
    "        estimated_total_time = elapsed_time / progress if progress > 0 else 0\n",
    "        remaining_time = estimated_total_time - elapsed_time\n",
    "           \n",
    "        pbar.set_postfix({\n",
    "            'progress': f'{processed_docs}/{total_docs}',\n",
    "            'remaining': f'{remaining_time:.1f}s'\n",
    "        })\n",
    "        pbar.update(len(batches[idx]))\n",
    "    \n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.community.pinecone import PineconeKiwiHybridRetriever\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "# 검색기 생성\n",
    "pinecone_params = {\"index\": index, \"namespace\": PINECONE_INDEX_NAME + \"-namespace-01\",\"embeddings\" :UpstageEmbeddings(model=\"solar-embedding-1-large-query\"),\"sparse_encoder\":sparse_encoder}\n",
    "pinecone_retriever = PineconeKiwiHybridRetriever(**pinecone_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 결과\n",
    "search_results = pinecone_retriever.invoke(\"차입금\")\n",
    "for result in search_results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 결과\n",
    "search_results = pinecone_retriever.invoke(\"환경\",search_kwargs={ \"top_n\": 3})\n",
    "for result in search_results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-TNmHyL-r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
