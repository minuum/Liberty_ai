{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PineconeRetrievalChain 테스트 노트북"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone API Key가 제대로 로드되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "if PINECONE_API_KEY:\n",
    "    print(\"Pinecone API Key가 제대로 로드되었습니다.\")\n",
    "else:\n",
    "    print(\"Pinecone API Key가 로드되지 않았습니다. .env 파일을 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 'liberty-rag-json'가 이미 존재합니다.\n",
      "Pinecone 인덱스가 성공적으로 초기화되었습니다: {'index': <pinecone.data.index.Index object at 0x163db8fd0>, 'namespace': 'liberty-rag-json-namespace-01'}\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Pinecone 인덱스 생성 및 초기화\n",
    "import pinecone_ as pinecone\n",
    "\n",
    "# 환경 변수 로드 (dotenv 파일 설정이 되어 있어야 합니다)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Pinecone API Key 및 인덱스 이름\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\", \"liberty-rag-json\")\n",
    "\n",
    "# Pinecone 인덱스 초기화\n",
    "retrieval_chain = pinecone.PineconeRetrievalChain(index_name=PINECONE_INDEX_NAME)\n",
    "print(f\"Pinecone 인덱스가 성공적으로 초기화되었습니다: {retrieval_chain.pinecone_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone Index 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Index, init\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME=os.getenv(\"PINECONE_INDEX_NAME\", \"liberty-ai-index\")\n",
    "def create_index(api_key: str=PINECONE_API_KEY, index_name: str=PINECONE_INDEX_NAME, dimension: int = 768, metric: str = \"dotproduct\", host=\"https://liberty-rag-json-hwsbh8f.svc.aped-4627-b74a.pinecone.io\"):\n",
    "    \"\"\"Pinecone 인덱스를 생성하고 반환합니다.\"\"\"\n",
    "    # Pinecone 클라이언트를 API 키로 초기화\n",
    "    #init(api_key=api_key,environment=PINECONE_ENVIRONMENT)\n",
    "    \n",
    "    try:\n",
    "            # 이미 존재하는 인덱스를 가져옴\n",
    "        index = Index(index_name, host=host)\n",
    "        print(f\"인덱스 '{index_name}'가 이미 존재합니다.\")\n",
    "    except Exception as e:\n",
    "        # 인덱스가 존재하지 않으면 새로 생성\n",
    "        print(f\"인덱스 '{index_name}'가 존재하지 않아서 새로 생성합니다.\")\n",
    "        from pinecone import create_index\n",
    "        create_index(\n",
    "                name=index_name,\n",
    "                dimension=dimension,  # 임베딩 차원수 (모델에 맞게 설정)\n",
    "                metric=metric\n",
    "            )\n",
    "        index = Index(index_name, host=host)\n",
    "        print(f\"새로운 인덱스 '{index_name}' 생성 완료\")\n",
    "        \n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #\"\"\"Pinecone 인덱스를 초기화하거나 존재하지 않으면 생성합니다.\"\"\"\n",
    "# host = \"https://liberty-index-hwsbh8f.svc.aped-4627-b74a.pinecone.io\"\n",
    "# index = create_index(api_key=PINECONE_API_KEY, host=host, index_name=PINECONE_INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 이름: {'dimension': 4096,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'liberty-rag-json-namespace-01': {'vector_count': 57104}},\n",
      " 'total_vector_count': 57104}\n",
      "네임스페이스: liberty-rag-json-namespace-01\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Pinecone 클라이언트 초기화\n",
    "host = \"https://liberty-rag-json-hwsbh8f.svc.aped-4627-b74a.pinecone.io\"\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# 인덱스 접근\n",
    "index = pc.Index(host=host)  # describe_index()로 host 확인 가능\n",
    "pinecone_params = {\"index\": index, \"namespace\": PINECONE_INDEX_NAME + \"-namespace-01\"}\n",
    "\n",
    "# 인덱스와 pinecone_params 테스트\n",
    "print(f\"인덱스 이름: {index.describe_index_stats()}\")\n",
    "print(f\"네임스페이스: {pinecone_params['namespace']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 업로드(json loading, split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 63704개 중 63704개의 문서를 로드했습니다.\n",
      "{'book_id': 'LJU000001', 'category': '환경/교통법', 'popularity': 3, 'keyword': ['개발제한구역의 지정 및 관리에 관한 특별조치법', '시정명령', '개발제한구역법', '원심판결을 파기', '도시계획법'], 'word_segment': 832, 'publication_ymd': '20131212'}\n",
      "이행강제금부과처분취소 (대법원 2013. 12. 12. 선고 2012두20397 판결) 【출전】 판례공보 제434호, 2014년 1월 15일 193페이지 【판시사항】 [1] 개발제한구역의 지정 및 관리에 관한 특별조치법상 이행강제금을 부과·징수할 때마다 그에 앞서 시정명령 절차를 다시 거쳐야 하는지 여부(소극) [2] 개발제한구역의 지정 및 관리에 관한 특별조치법에 의한 이행강제금 부과의 근거가 되는 시정명령이 이루어져야 하는 시기(=법률 시행일인 2010. 2. 7. 이후) 【판결요지】 [1] 개발제한구역의 지정 및 관리에 관한 특별조치법 제30조 제1항, 제30조의2 제1항 및 제2항의 규정에 의하면 시정명령을 받은 후 그 시정명령의 이행을 하지 아니한 자에 대하여 이행강제금을 부과할 수 있고, 이행강제금을 부과하기 전에 상당한 기간을 정하여 그 기한까지 이행되지 아니할 때에 이행강제금을 부과·징수한다는 뜻을 문서로 계고하여야 하므로, 이행강제금의 부과·징수를 위한 계고는 시정명령을 불이행한 경우에 취할 수 있는 절차라 할 것이고, 따라서 이행강제금을 부과·징수할 때마다 그에 앞서 시정명령 절차를 다시 거쳐야 할 필요는 없다. [2] 구 개발제한구역의 지정 및 관리에 관한 특별조치법(2009. 2. 6. 법률 제9436호로 개정되기 전의 것)이나 개발제한구역의 지정 및 관리에 관한 특별조치법(이하 '개발제한구역법'이라 한다)이 제정·시행되기 전의 구 도시계획법(2000. 1. 28. 법률 제6243호로 전부 개정되기 전의 것)은 개발제한구역에서 행위제한을 위반한 자에 대한 시정명령을 정하고 있을 뿐이었으나, 2009. 2. 6. 법률 제9436호로 개발제한구역법을 개정하면서 시정명령을 이행하지 아니한 자에 대한 이행강제금을 정한 개발제한구역법 제30조의2를 신설하는 한편 그 이행강제금 부과의 근거가 되는 시정명령에 관한 제30조를 개정하였는데, 건축물·공작물 등의 철거·폐쇄·개축 또는 이전에 관하여는 시정명령의 요건이나 내용이 변경되었을 뿐만 아니라 종전 규정과 달리 '상당한 기간을 정하여' 시정명령을 하도록 하였다. 그리고 위 법률 부칙은 제1조에서 \"이 법은 공포 후 6개월이 경과한 날부터 시행한다. 다만, 제30조 및 제30조의2의 개정규정은 공포 후 1년이 경과한 날부터 시행한다.\"고 규정하여 신설된 이행강제금 규정과 그 이행강제금 부과의 근거가 되는 시정명령에 관한 개정규정이 2010. 2. 7. 함께 시행되도록 하고 있으며, 달리 개정 법률 시행 당시 종전의 규정에 따라 이루어진 시정명령 등에 관한 일반적인 경과조치 규정을 두고 있지 않다. 위와 같은 개발제한구역법의 개정 경과 및 규정 내용 등에 비추어 보면, 개발제한구역법에 의한 이행강제금 부과의 근거가 되는 시정명령은 위 법률 시행일인 2010. 2. 7. 이후에 이루어져야 한다. 【참조조문】 [1] 개발제한구역의 지정 및 관리에 관한 특별조치법 제30조 제1항, 제30조의2제1항, 제2항 [2] 개발제한구역의 지정 및 관리에 관한 특별조치법 제30조 제1항, 제30조의2제1항, 부칙(2009.2.6.)제1조 구 도시계획법(2000.1.28. 법률 제6243호로 전부개정되기 전의 것)제4조 제5항 【원심판결】 부산고법 2012.8.22. 선고 2012누379판결 【주문】 원심판결을 파기하고, 사건을 부산고등법원으로 환송한다. 【이유】 상고이유를 판단한다. 1. 개발제한구역의 지정 및 관리에 관한 특별조치법(이하 '개발제한구역법'이라 한다)제30조 제1항, 제30조의2제1항 및 제2항의 규정에 의하면 시정명령을 받은 후 그 시정명령의 이행을 하지 아니한 자에 대하여 이행강제금을 부과할 수 있고, 그 이행강제금을 부과하기 전에 상당한 기간을 정하여 그 기한까지 이행되지 아니할 때에 이행강제금을 부과·징수한다는 뜻을 문서로 계고하여야 하므로, 이행강제금의 부과·징수를 위한 계고는 시정명령을 불이행한 경우에 취할 수 있는 절차라 할 것이고, 따라서 이행강제금을 부과·징수할 때마다 그에 앞서 시정명령 절차를 다시 거쳐야 할 필요는 없다고 보아야 한다. 한편 구 개발제한구역법(2009.2.6. 법률 제9436호로 개정되기 전의 것)이나 개발제한구역법이 제정·시행되기 전의 구 도시계획법(2000.1.28. 법률 제6243호로 전부 개정되기 전의 것, 이하 같다)은 개발제한구역에서의 행위제한을 위반한 자에 대한 시정명령을 정하고 있을 뿐이었으나, 2009.2.6. 법률 제9436호로 개발제한구역법을 개정하면서 시정명령을 이행하지 아니한 자에 대한 이행강제금을 정한 개발제한구역법 제30조의2를 신설하는 한편 그 이행강제금 부과의 근거가 되는 시정명령에 관한 제30조를 개정하였는데, 건축물·공작물 등의 철거·폐쇄·개축 또는 이전에 관하여는 시정명령의 요건이나 내용이 변경되었을 뿐만 아니라 종전의 규정과 달리 '상당한 기간을 정하여'시정명령을 하도록 하였다. 그리고 위 법률 부칙은 제1조에서 \"이 법은 공포 후 6개월이 경과한 날부터 시행한다. 다만, 제30조 및 제30조의2의 개정규정은 공포 후 1년이 경과한 날부터 시행한다.\"고 규정하여 신설된 이행강제금 규정과 그 이행강제금 부과의 근거가 되는 시정명령에 관한 개정규정이 2010.2.7. 함께 시행되도록 하고 있으며, 달리 개정 법률 시행 당시 종전의 규정에 따라 이루어진 시정명령 등에 관한 일반적인 경과조치 규정을 두고 있지 않다. 위와 같은 개발제한구역법의 개정 경과 및 규정 내용 등에 비추어 보면, 개발제한구역법에 의한 이행강제금 부과의 근거가 되는 시정명령은 위 법률 시행일인 2010.2.7. 이후에 이루어져야 한다. 2. 원심은, 판시와 같은 사정에 비추어 보면 ① 피고가 2000.4.28. 원고에게 보낸 '개발제한구역 내 불법행위 원상복구 계고'라는 제목의 문서는 개발제한구역법 제30조 제1항에서 정하고 있는 시정명령에 해당하고, ② 피고가 2010.10.18. 원고에게 보낸 '2010. 정기분이행강제금 시정명령 및 부과예고 처분'이라는 제목의 문서는 개발제한구역법 제30조의2제2항에 따른 계고로 보아야 하므로, 피고의 이 사건 이행강제금 부과는 적법한 절차를 거쳐 이루어진 것이라고 판단하여, 그 취소를 명한 제1심판결을 취소하고 원고의 청구를 기각하는 판결을 선고하였다. 3. 원심판결 이유를 앞서 본 법리에 비추어 살펴보면, 원심이 이 사건 이행강제금 부과의 근거가 되는 시정명령으로 들고 있는 위 2000.4.28. 자 문서는 구 도시계획법에 따라 이루어진 것에 불과하여 개발제한구역법 제30조의2제1항이 정한 '제30조 제1항에 따른 시정명령'에 해당한다고 보기 어렵고, 원심판결 이유 및 기록에 비추어 보아도 피고가 2010.2.7. 부터 위 2010.10.18. 자 문서를 보내기까지 사이에 피고에게 시정명령을 하였다는 사정은 나타나 있지 않다. 그렇다면 위 2010.10.18. 자 문서에 의한 계고 및 그에 기초한 이 사건 이행강제금 부과는 그 근거가 될 수 있는 적법한 시정명령 절차를 거치지 않고 이루어진 것으로서 위법하다 할 것이다. 따라서 이와 결론을 달리한 원심의 판단에는 상고이유 주장과 같이 이행강제금 부과에 필요한 시정명령의 요건에 관한 법리를 오해하여 판결에 영향을 미친 위법이 있다. 4. 결론 그러므로 원심판결을 파기하고, 사건을 다시 심리·판단하도록 원심법원에 환송하기로 하여 관여 대법관의 일치된 의견으로 주문과 같이 판결한다.\n",
      "LJU000001\n",
      "환경/교통법\n",
      "3\n",
      "['개발제한구역의 지정 및 관리에 관한 특별조치법', '시정명령', '개발제한구역법', '원심판결을 파기', '도시계획법']\n",
      "832\n",
      "20131212\n"
     ]
    }
   ],
   "source": [
    "# Test 2: 문서 로드 및 전처리 테스트\n",
    "import pinecone_\n",
    "import os\n",
    "\n",
    "# 데이터 경로 설정\n",
    "data_dir = \"data/154.의료, 법률 전문 서적 말뭉치/01-1.정식개방데이터/Training/02.라벨링데이터/Training_legal.json\"  # data 폴더에 PDF 파일들을 저장해 주세요\n",
    "import json\n",
    "split_docs=[]\n",
    "with open(data_dir, 'r', encoding='utf-8') as f:\n",
    "    try:\n",
    "        json_data = json.load(f)\n",
    "                    \n",
    "        # data 배열에서 문서 추출 (처음 10개만)\n",
    "        if isinstance(json_data, dict) and 'data' in json_data:\n",
    "            for item in json_data['data']:  # 처음 10개의 문서만 로드\n",
    "                if isinstance(item, dict):\n",
    "                    # Document 객체 생성\n",
    "                    from langchain.schema import Document\n",
    "                    doc = Document(\n",
    "                        page_content=item.get('text', ''),\n",
    "                        metadata={\n",
    "                            'book_id': item.get('book_id'),\n",
    "                            'category': item.get('category'),\n",
    "                            'popularity': item.get('popularity'),\n",
    "                            'keyword': item.get('keyword', []),\n",
    "                            'word_segment': item.get('word_segment', []),\n",
    "                            'publication_ymd': item.get('publication_ymd')\n",
    "                        }\n",
    "                    )\n",
    "                    split_docs.append(doc)\n",
    "                else:\n",
    "                    print(\"JSON 데이터가 예상된 형식이 아닙니다.\")\n",
    "                    print(\"데이터 구조:\", json_data.keys() if isinstance(json_data, dict) else type(json_data))\n",
    "            \n",
    "            print(f\"전체 {len(json_data['data'])}개 중 {len(split_docs)}개의 문서를 로드했습니다.\")\n",
    "                        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON 파일 파싱 중 오류가 발생했습니다: {e}\")\n",
    "print(split_docs[0].metadata)\n",
    "print(split_docs[0].page_content)\n",
    "print(split_docs[0].metadata['book_id'])\n",
    "print(split_docs[0].metadata['category'])\n",
    "print(split_docs[0].metadata['popularity'])\n",
    "print(split_docs[0].metadata['keyword'])\n",
    "print(split_docs[0].metadata['word_segment'])\n",
    "print(split_docs[0].metadata['publication_ymd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "from tqdm import tqdm\n",
    "def preprocess_documents(\n",
    "    split_docs: List[Any], \n",
    "    metadata_keys: List[str] = [\"book_id\", \"category\", \"popularity\", \"keyword\", \"word_segment\", \"publication_ymd\"], \n",
    "    min_length: int = 5\n",
    "):\n",
    "    \"\"\"문서를 전처리하고 내용과 메타데이터를 반환합니다.\"\"\"\n",
    "    contents = []\n",
    "    metadatas = {key: [] for key in metadata_keys}\n",
    "    \n",
    "    for doc in tqdm(split_docs, desc=\"문서 전처리 중\"):\n",
    "        content = doc.page_content.strip()\n",
    "        if content and len(content) >= min_length:\n",
    "            # 컨텍스트 길이 제한 (Pinecone 권장사항)\n",
    "            contents.append(content[:4000])  \n",
    "            \n",
    "            # 메타데이터 추출\n",
    "            for k in metadata_keys:\n",
    "                value = doc.metadata.get(k)\n",
    "                metadatas[k].append(value)\n",
    "                \n",
    "    print(f\"전체 {len(split_docs)}개 중 {len(contents)}개의 문서가 전처리되었습니다.\")\n",
    "    return contents, metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "문서 전처리 중: 100%|██████████| 63704/63704 [00:00<00:00, 182826.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 63704개 중 63704개의 문서가 전처리되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contents, metadatas = preprocess_documents(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_teddynote.korean import stopwords\n",
    "\n",
    "# 한글 불용어 사전 불러오기 (불용어 사전 출처: https://www.ranks.nl/stopwords/korean)\n",
    "stopword = stopwords()\n",
    "#stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.community.pinecone import (\n",
    "    create_sparse_encoder,\n",
    "    fit_sparse_encoder,\n",
    ")\n",
    "\n",
    "# 한글 불용어 사전 + Kiwi 형태소 분석기를 사용합니다.\n",
    "sparse_encoder = create_sparse_encoder(stopwords(), mode=\"kiwi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034d74fd39044d8abc58bb7fa0a1d231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit_sparse_encoder]\n",
      "Saved Sparse Encoder to: ./sparse_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Sparse Encoder 를 사용하여 contents 를 학습\n",
    "saved_path = fit_sparse_encoder(\n",
    "    sparse_encoder=sparse_encoder, contents=contents, save_path=\"./sparse_encoder.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_sparse_encoder]\n",
      "Loaded Sparse Encoder from: ./sparse_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.community.pinecone import load_sparse_encoder\n",
    "\n",
    "# 추후에 학습된 sparse encoder 를 불러올 때 사용합니다.\n",
    "sparse_encoder = load_sparse_encoder(\"./sparse_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_teddynote.community.kiwi_tokenizer import KiwiBM25Tokenizer\n",
    "passage_embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large-query\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Sparse Encoder 학습 및 로드 \n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import secrets\n",
    "import itertools\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Index, init, Pinecone\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from typing import List, Dict, Any\n",
    "from langchain_teddynote.community.kiwi_tokenizer import KiwiBM25Tokenizer\n",
    "def generate_hash() -> str:\n",
    "    \"\"\"24자리 무작위 hex 값을 생성하고 6자리씩 나누어 '-'로 연결합니다.\"\"\"\n",
    "    random_hex = secrets.token_hex(12)\n",
    "    return \"-\".join(random_hex[i: i + 6] for i in range(0, 24, 6))\n",
    "\n",
    "from pinecone import Index, init\n",
    "import itertools\n",
    "def chunks(iterable, size):\n",
    "    it = iter(iterable)\n",
    "    chunk = list(itertools.islice(it, size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = list(itertools.islice(it, size))\n",
    "\n",
    "# def upsert_documents_parallel(contents, metadatas, sparse_encoder, pinecone_params, embedder=UpstageEmbeddings(model=\"solar-embedding-1-large-query\"), batch_size=100, max_workers=30):\n",
    "#     # 문자열을 Document 객체로 변환\n",
    "from langchain.schema import Document\n",
    "keys = list(metadatas.keys())\n",
    "embedder=passage_embeddings\n",
    "    \n",
    "    # documents = [\n",
    "    #     Document(\n",
    "    #         page_content=text,\n",
    "    #         metadata={\n",
    "    #             'book_id': metadatas['book_id'][i],\n",
    "    #             'category': metadatas['category'][i], \n",
    "    #             'popularity': metadatas['popularity'][i],\n",
    "    #             'keyword': metadatas['keyword'][i],\n",
    "    #             'word_segment': metadatas['word_segment'][i],\n",
    "    #             'publication_ymd': metadatas['publication_ymd'][i]\n",
    "    #         }\n",
    "    #     ) for i, text in enumerate(batch)\n",
    "    # ]\n",
    "\n",
    "def process_batch(batch, contents, metadatas, sparse_encoder, passage_embeddings, pinecone_params):\n",
    "    \"\"\"\n",
    "    배치 단위로 문서를 처리하여 Pinecone에 업로드합니다.\n",
    "    \n",
    "    Args:\n",
    "        batch: 현재 처리할 문서들의 인덱스 리스트\n",
    "        contents: 전체 문서 내용 리스트\n",
    "        metadatas: 전체 문서의 메타데이터 딕셔너리\n",
    "        sparse_encoder: 희소 임베딩을 생성하는 인코더\n",
    "        passage_embeddings: 밀집 임베딩을 생성하는 인코더\n",
    "        pinecone_params: Pinecone 관련 설정\n",
    "    \"\"\"\n",
    "    # 현재 배치에 해당하는 문서와 메타데이터 추출\n",
    "    context_batch = [contents[i] for i in batch]\n",
    "    metadata_keys = ['book_id', 'category', 'popularity', 'keyword', 'word_segment', 'publication_ymd']\n",
    "    \n",
    "    batch_result = [\n",
    "        {\n",
    "            \"context\": context[:1000],  # 컨텍스트 길이 제한\n",
    "            **{key: metadatas[key][i] for key in metadata_keys}\n",
    "        } for i, context in enumerate(context_batch)\n",
    "    ]\n",
    "\n",
    "    ids = [generate_hash() for _ in range(len(batch))]\n",
    "    dense_embeds = passage_embeddings.embed_documents(context_batch)\n",
    "    sparse_embeds = sparse_encoder.encode_documents(context_batch)\n",
    "    print(f\"Processing batch: {batch}\")\n",
    "    vectors = [\n",
    "        {\n",
    "            \"id\": id_,\n",
    "            \"values\": dense_embed,\n",
    "            \"sparse_values\": sparse_embed,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        for id_, dense_embed, sparse_embed, metadata in zip(\n",
    "            ids, dense_embeds, sparse_embeds, batch_result\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        return index.upsert(\n",
    "            vectors=vectors,\n",
    "            namespace=pinecone_params[\"namespace\"],\n",
    "            async_req=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Upsert 중 오류 발생: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5effbc5df284108882ad810dc98bff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "문서 Upsert 중: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 0개의 Vector가 Upsert 되었습니다.\n",
      "{'dimension': 4096,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'liberty-rag-json-namespace-01': {'vector_count': 57004}},\n",
      " 'total_vector_count': 57004}\n"
     ]
    }
   ],
   "source": [
    "batch_size=100\n",
    "max_workers=30\n",
    "# 57,005번째 문서부터 처리\n",
    "start_idx = 57005\n",
    "contents_subset = contents[start_idx:]\n",
    "metadata_subset = {key: values[start_idx:] for key, values in metadatas.items()}\n",
    "\n",
    "batches = list(chunks(range(len(contents_subset)), batch_size))\n",
    "print(batches)\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            process_batch, \n",
    "            batch,\n",
    "            contents_subset,\n",
    "            metadata_subset,\n",
    "            sparse_encoder,\n",
    "            passage_embeddings,\n",
    "            pinecone_params\n",
    "        ) for batch in batches\n",
    "    ]\n",
    "    print(futures)\n",
    "    results = []\n",
    "    \n",
    "    # tqdm 설정 추가\n",
    "    total_docs = len(contents_subset)\n",
    "    pbar = tqdm(total=total_docs, desc=\"문서 Upsert 중\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    processed_docs = 0\n",
    "    for idx, future in enumerate(as_completed(futures)):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            \n",
    "        # 진행률과 예상 시간 계산\n",
    "        elapsed_time = time.time() - start_time\n",
    "        processed_docs += len(batches[idx])\n",
    "        progress = processed_docs / total_docs\n",
    "        estimated_total_time = elapsed_time / progress if progress > 0 else 0\n",
    "        remaining_time = estimated_total_time - elapsed_time\n",
    "        \n",
    "        # 진행 상태 업데이트\n",
    "        pbar.set_postfix({\n",
    "            'progress': f'{processed_docs}/{total_docs}',\n",
    "            'remaining': f'{remaining_time:.1f}s'\n",
    "        })\n",
    "        pbar.update(len(batches[idx]))\n",
    "    \n",
    "    pbar.close()\n",
    "    total_upserted = sum(result.upserted_count for result in results if result)\n",
    "    print(f\"총 {total_upserted}개의 Vector가 Upsert 되었습니다.\")\n",
    "    print(f\"{pinecone_params['index'].describe_index_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents subset length: 700\n",
      "Metadata subset length: 6\n",
      "Number of batches: 7\n",
      "Number of futures: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch: [400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499]\n",
      "Processing batch: [600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch: [300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "문서 Upsert 중:  14%|█▍        | 100/700 [01:03<06:06,  1.64it/s, progress=200/700, remaining=156.2s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "Processing batch: [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "문서 Upsert 중:  71%|███████▏  | 500/700 [01:18<00:18, 10.79it/s, progress=500/700, remaining=31.1s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch: [500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "문서 Upsert 중: 100%|██████████| 700/700 [01:20<00:00,  8.67it/s, progress=700/700, remaining=0.0s]\n"
     ]
    }
   ],
   "source": [
    "# 57,005번째 문서부터 처리하기 위한 데이터 슬라이싱\n",
    "start_idx = min(63004, len(contents))\n",
    "contents_subset = contents[start_idx:]\n",
    "metadata_subset = {key: values[start_idx:] for key, values in metadatas.items()}\n",
    "print(f\"Contents subset length: {len(contents_subset)}\")\n",
    "print(f\"Metadata subset length: {len(metadata_subset)}\")\n",
    "batch_size = 100\n",
    "max_workers = 30\n",
    "\n",
    "# 배치 생성\n",
    "batches = list(chunks(range(len(contents_subset)), batch_size))\n",
    "print(f\"Number of batches: {len(batches)}\")\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [\n",
    "        executor.submit(\n",
    "            process_batch, \n",
    "            batch,\n",
    "            contents_subset,  # 슬라이싱된 contents 사용\n",
    "            metadata_subset,  # 슬라이싱된 metadata 사용\n",
    "            sparse_encoder,\n",
    "            passage_embeddings,\n",
    "            pinecone_params\n",
    "        ) for batch in batches\n",
    "    ]\n",
    "    print(f\"Number of futures: {len(futures)}\")\n",
    "    # 진행 상황 모니터링\n",
    "    total_docs = len(contents_subset)\n",
    "    pbar = tqdm(total=total_docs, desc=\"문서 Upsert 중\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = []\n",
    "    processed_docs = 0\n",
    "    \n",
    "    for idx, future in enumerate(as_completed(futures)):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            \n",
    "        processed_docs += len(batches[idx])\n",
    "        elapsed_time = time.time() - start_time\n",
    "        progress = processed_docs / total_docs\n",
    "        estimated_total_time = elapsed_time / progress if progress > 0 else 0\n",
    "        remaining_time = estimated_total_time - elapsed_time\n",
    "           \n",
    "        pbar.set_postfix({\n",
    "            'progress': f'{processed_docs}/{total_docs}',\n",
    "            'remaining': f'{remaining_time:.1f}s'\n",
    "        })\n",
    "        pbar.update(len(batches[idx]))\n",
    "    \n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 네임스페이스삭제\n",
    "pinecone_params['index'].delete(namespace=pinecone_params['namespace'])\n",
    "print(f\"네임스페이스 '{pinecone_params['namespace']}'가 성공적으로 삭제되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-TNmHyL-r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
